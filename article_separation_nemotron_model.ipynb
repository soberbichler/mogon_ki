{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGa0deRVp3KpKNUnjXiUKr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soberbichler/mogon_ki/blob/main/article_separation_nemotron_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BfyThlPcS7x"
      },
      "outputs": [],
      "source": [
        "import ollama"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['no_proxy'] = 'localhost'\n"
      ],
      "metadata": {
        "id": "xjFPgObKcjty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  client = ollama.Client()\n",
        "  client.list()"
      ],
      "metadata": {
        "id": "q5C6sP_WcmY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Replace 'your_excel_file.xlsx' with the actual path to your Excel file\n",
        "df = pd.read_excel('your_excel_file.xlsx')\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "5qVqySaiizWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from typing import List, Dict\n",
        "\n",
        "with open('examples.txt', 'r') as file:\n",
        "    examples = file.read()\n",
        "\n",
        "def analyze_dataframe(df: pd.DataFrame, text_column: str) -> pd.DataFrame:\n",
        "    def analyze_text(text: str) -> List[Dict[str, str]]:\n",
        "        combined_prompt = f\"\"\"\n",
        "# System Instructions\n",
        "You are an expert text analyst and information retrieval specialist. Your task is to carefully analyze given texts and extract complete articles that contain specific themes. Follow these guidelines and use {examples} to learn from:\n",
        "\n",
        "1. Approach each text with meticulous attention to detail.\n",
        "2. Identify all instances of the specified theme within the text.\n",
        "3. For each keyword occurrence:\n",
        "   a. Determine the beginning of the article containing the keyword.\n",
        "   b. Analyze sentence by sentence to ensure continuity and relevance.\n",
        "   c. Include the entire article, regardless of length. Do not truncate or summarize. Never cut an article, even if not every sentence is on earthquake\n",
        "   d. If articles have headlines, consider them as start/end markers.\n",
        "4. Verify each extracted article:\n",
        "   a. Ensure it forms a coherent unit.\n",
        "   b. Confirm it contains the specified keyword.\n",
        "   c. Check for completeness and inclusion of all relevant information.\n",
        "5. Extract and present each verified article in its original, unaltered form.\n",
        "6. Separate distinct articles clearly.\n",
        "7. If no articles containing the keyword are found, state this explicitly.\n",
        "\n",
        "Your output should consist solely of the extracted articles or the statement that no relevant articles were found. Do not include explanations, summaries, or additional commentary unless specifically requested.\n",
        "\n",
        "Maintain a neutral, objective stance throughout the analysis. Focus on accuracy and completeness in your extractions.\n",
        "\n",
        "# Task Instructions\n",
        "Bitte führe die folgenden Schritte aus und orientiere dich an folgenden Beispielen {examples}\n",
        "1. Lese den gesamten Text sorgfältig durch.\n",
        "2.  Identifiziere alle Artikel zum Thema Erdbeben, Erdbebenkatastrophe in Italien/Messina/Sizilien/katastrophe/Trümmer und Flüchtlinge und suche nach Keywords 'Erdbeben', 'Erdstoß', Erdbebenkatastrophe', 'Messina', 'Trümmer', 'Hilfsaktion', 'Katastrophe'.\n",
        "3. Für jedes Vorkommen des themas:\n",
        "   a. Bestimme den Anfang des Artikels, in dem KEywords vorkommen.\n",
        "   b. Kontrolliere Satz für Satz, ob diese zusammengehören, Ende den Artikel, wenn die Sätze nicht mehr zusammengehören.\n",
        "   c. Berücksichtige auch lange Artikel, kürze keine Artikel, keine Artikel sprengen den Rahmen der Antwort\n",
        "   d. Artikel mit Überschriften starten mit überschrift und Enden mit Überschrift des nächsten Artikels\n",
        "   c. Markiere den vollständigen Artikel von Anfang bis Ende.\n",
        "4. Überprüfe jeden markierten Artikel:\n",
        "   a. Stelle sicher, dass er eine zusammenhängende Einheit bildet.\n",
        "   b. Vergewissere dich, dass er zum Thema passt.\n",
        "   c. Prüfe, ob er vollständig ist und keine wichtigen Informationen fehlen.\n",
        "5. Extrahiere jeden überprüften Artikel als Originaltext ohne Korrekturen.\n",
        "6. Gib die extrahierten Artikel exakt und unverändert wieder, ohne Zusammenfassungen oder zusätzliche Kommentare.\n",
        "7. Wenn keine Artikel gefunden wurden, gib \"Keine Artikel mit dem angegebenen Keyword gefunden.\" aus.\n",
        "8. Trenne jeden Artikel mit einem speziellen Trennzeichen, z.B. \"###ARTICLE_SEPARATOR###\".\n",
        "\n",
        "Führe nun diese Schritte für den folgenden Text aus:\n",
        "{text}\n",
        "\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = client.generate(\n",
        "                model='nemotron:latest',\n",
        "                prompt=combined_prompt,\n",
        "                options={\n",
        "                    'num_ctx': 50000,\n",
        "                    'temperature': 0.4,\n",
        "                    'top_p': 0.5,\n",
        "                    'num_predict': 20048,\n",
        "                    'repeat_penalty': 1.1,\n",
        "                    'top_k': 40\n",
        "                }\n",
        "            )\n",
        "\n",
        "            articles = response['response'].split(\"###ARTICLE_SEPARATOR###\")\n",
        "            return [{\"article\": article.strip()} for article in articles if article.strip()]\n",
        "        except Exception as e:\n",
        "            print(f\"Error in AI processing: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    # Apply the analysis to each row in the DataFrame\n",
        "    all_articles = []\n",
        "    for index, row in df.iterrows():\n",
        "        articles = analyze_text(row[text_column])\n",
        "        for article in articles:\n",
        "            new_row = row.to_dict()\n",
        "            new_row['extracted_article'] = article['article']\n",
        "            all_articles.append(new_row)\n",
        "\n",
        "    # Create a new DataFrame with individual rows for each article\n",
        "    result_df = pd.DataFrame(all_articles)\n",
        "\n",
        "    return result_df\n",
        "\n",
        "# Usage example (run this in your notebook)\n",
        "text_column = 'plainpagefulltext'\n",
        "result_df = analyze_dataframe(df, text_column)\n",
        "\n",
        "# Optionally, save the results to an Excel file\n",
        "result_df.to_excel('analysis_results.xlsx', index=False)\n",
        "\n",
        "# Display the first few rows of the result\n",
        "print(result_df.head())\n"
      ],
      "metadata": {
        "id": "WEkXVF7GhCSq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}