{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTNVFR3s4YagtrJ78l+N/z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soberbichler/mogon_ki/blob/main/article_separation_nemotron_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BfyThlPcS7x"
      },
      "outputs": [],
      "source": [
        "import ollama"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['no_proxy'] = 'localhost'\n"
      ],
      "metadata": {
        "id": "xjFPgObKcjty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  client = ollama.Client()\n",
        "  client.list()"
      ],
      "metadata": {
        "id": "q5C6sP_WcmY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Replace 'your_excel_file.xlsx' with the actual path to your Excel file\n",
        "df = pd.read_excel('your_excel_file.xlsx')\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "5qVqySaiizWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from typing import List, Dict\n",
        "\n",
        "with open('examples.txt', 'r') as file:\n",
        "    examples = file.read()\n",
        "\n",
        "def analyze_dataframe(df: pd.DataFrame, text_column: str) -> pd.DataFrame:\n",
        "    def analyze_text(text: str) -> List[Dict[str, str]]:\n",
        "        system_prompt = f\"\"\"\n",
        "# System Instructions\n",
        "You are an expert text analyst and information retrieval specialist. Use {examples} for structuring your answer.\n",
        "Your task is to carefully analyze given texts and extract complete articles that contain specific themes. You never change original texts.\n",
        "When analyzing texts for earthquake-related content, look for:\n",
        "1. Direct mentions of seismic events\n",
        "2. Scientific measurements and observations\n",
        "3. Location and time details\n",
        "4. Impact descriptions\n",
        "5. Official reports from observatories\n",
        "6. Related natural phenomena\n",
        "\n",
        "Classify as relevant if the text contains:\n",
        "- Primary earthquake terminology\n",
        "- Specific seismic measurements\n",
        "- Official earthquake reports\n",
        "- Impact descriptions\n",
        "\n",
        "Your output should consist of the extracted articles, the verification and explanation.\n",
        "\n",
        "Maintain a neutral, objective stance throughout the analysis. Focus on accuracy and completeness in your extractions.\"\"\"\n",
        "\n",
        "        user_prompt = f\"\"\"\n",
        "\n",
        "# Task Instructions\n",
        "Bitte führe die folgenden Schritte aus\n",
        "1. Lese jeden Text aufmerksam durch. Behandle jeden Text als eigene Einheit, ohne auf andere TExte zu refierieren\n",
        "2. Identifiziere alle Artikel zum Thema Erdbeben\n",
        "3. Für jedes Vorkommen des Themas:\n",
        "   a. Bestimme den Anfang des Artikels, in dem das Thema vorkommen.\n",
        "   b. Kontrolliere Satz für Satz, ob diese zusammengehören, Ende den Artikel, wenn die Sätze nicht mehr zusammengehören.\n",
        "   c. Markiere den vollständigen Artikel von Anfang bis Ende.\n",
        "   d. Wenn der Artikel zu lang für eine Antwort ist, schreibe \"[CONTINUED]\" am Ende und setze in der nächsten Antwort fort.\n",
        "    e. Berücksichtige auch sehr kurze und sehr lange Artikel\n",
        "4. Überprüfe jeden markierten Artikel:\n",
        "   a. Stelle sicher, dass er eine Einheit bildet, auch wenn es nicht mehr um Erdbeben geht.\n",
        "   b. Vergewissere dich, dass er eines der gennanten Themen enthält.\n",
        "   c. Prüfe, ob der extrahierte Text tatsächlich im Dokument ist\n",
        "5. Extrahiere jeden überprüften Artikel als Originaltext, der nichts als den originalen Text enthält, auch keine OCR Korrekturen\n",
        "6. Erkläre deine Vorgehen: Decision Trees, Logistic Regression\n",
        "7. Wenn keine Artikel gefunden wurden, gib \"Keine Artikel mit dem angegebenen Thema gefunden.\" aus.\n",
        "\n",
        "Führe nun diese Schritte für den folgenden Text aus:\n",
        "{text}\n",
        "\"\"\"\n",
        "\n",
        "        articles = []\n",
        "        current_article = \"\"\n",
        "        continuation = False\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                messages = [\n",
        "                       {\n",
        "                            'role': 'system',\n",
        "                            'content': system_prompt\n",
        "                        },\n",
        "                        {\n",
        "                            'role': 'user',\n",
        "                            'content': user_prompt if not continuation else current_article\n",
        "                        }\n",
        "                ]\n",
        "\n",
        "                response = client.chat(\n",
        "                    model='nemotron:latest',\n",
        "                    messages = messages,\n",
        "                    options={\n",
        "                         'num_ctx': 30000,\n",
        "                        'temperature': 0.0,\n",
        "                        'num_predict': 20000,\n",
        "                        'repeat_penalty': 1,\n",
        "                        'top_k': 20\n",
        "                    }\n",
        "                )\n",
        "\n",
        "                content = response.get('message', {}).get('content', '')\n",
        "\n",
        "                if continuation:\n",
        "                    current_article += content\n",
        "                else:\n",
        "                    parts = content.split(\"**END OF ARTICLE**\")\n",
        "                    for part in parts[:-1]:\n",
        "                        if current_article:\n",
        "                            articles.append({\"article\": current_article.strip()})\n",
        "                            current_article = \"\"\n",
        "                        articles.append({\"article\": part.strip()})\n",
        "                    current_article = parts[-1]\n",
        "\n",
        "                if content.endswith(\"[CONTINUED]\"):\n",
        "                    continuation = True\n",
        "                    current_article = current_article[:-11]  # Remove \"[CONTINUED]\"\n",
        "                else:\n",
        "                    continuation = False\n",
        "                    if current_article:\n",
        "                        articles.append({\"article\": current_article.strip()})\n",
        "                        current_article = \"\"\n",
        "                    break\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in AI processing: {str(e)}\")\n",
        "                break\n",
        "\n",
        "        return articles\n",
        "\n",
        "    # Apply the analysis to each row in the DataFrame\n",
        "    all_articles = []\n",
        "    for index, row in df.iterrows():\n",
        "        articles = analyze_text(row[text_column])\n",
        "        for i, article in enumerate(articles, 1):\n",
        "            new_row = row.to_dict()\n",
        "            new_row['extracted_article'] = article['article']\n",
        "            new_row['article_part'] = i\n",
        "            new_row['total_parts'] = len(articles)\n",
        "            all_articles.append(new_row)\n",
        "\n",
        "    # Create a new DataFrame with individual rows for each article\n",
        "    result_df = pd.DataFrame(all_articles)\n",
        "\n",
        "    return result_df\n",
        "\n",
        "# Usage example (run this in your notebook)\n",
        "text_column = 'plainpagefulltext'\n",
        "result_df = analyze_dataframe(df, text_column)\n",
        "\n",
        "# Optionally, save the results to an Excel file\n",
        "result_df.to_excel('analysis_results.xlsx', index=False)\n",
        "\n",
        "# Display the first few rows of the result\n",
        "print(result_df.head())"
      ],
      "metadata": {
        "id": "WEkXVF7GhCSq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}