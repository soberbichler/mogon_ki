{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMx3vxXOtzVMw0UrSg/CvHK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soberbichler/mogon_ki/blob/main/article_separation_nemotron_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BfyThlPcS7x"
      },
      "outputs": [],
      "source": [
        "import ollama"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['no_proxy'] = 'localhost'\n"
      ],
      "metadata": {
        "id": "xjFPgObKcjty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  client = ollama.Client()\n",
        "  client.list()"
      ],
      "metadata": {
        "id": "q5C6sP_WcmY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Replace 'your_excel_file.xlsx' with the actual path to your Excel file\n",
        "df = pd.read_excel('your_excel_file.xlsx')\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "5qVqySaiizWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from typing import List, Dict\n",
        "\n",
        "with open('examples.txt', 'r') as file:\n",
        "    examples = file.read()\n",
        "\n",
        "def analyze_dataframe(df: pd.DataFrame, text_column: str) -> pd.DataFrame:\n",
        "    def analyze_text(text: str) -> List[Dict[str, str]]:\n",
        "        combined_prompt = f\"\"\"\n",
        "# System Instructions\n",
        "You are an expert text analyst and information retrieval specialist. Your task is to carefully analyze given texts and extract complete articles that contain specific themes. Follow these guidelines and use {examples} to learn from:\n",
        "\n",
        "1. Approach each text with meticulous attention to detail.\n",
        "2. Identify all instances of the specified theme within the text.\n",
        "3. For each keyword occurrence:\n",
        "   a. Determine the beginning of the article containing the keyword.\n",
        "   b. Analyze sentence by sentence to ensure continuity and relevance.\n",
        "   c. Include the entire article. If the article is too long to fit in one response, write \"[CONTINUED]\" at the end and continue in the next response.\n",
        "   d. If articles have headlines, consider them as start/end markers.\n",
        "4. Verify each extracted article:\n",
        "   a. Ensure it forms a coherent unit.\n",
        "   b. Confirm it contains the specified keyword.\n",
        "   c. Check for completeness and inclusion of all relevant information.\n",
        "5. Extract and present each verified article in its original, unaltered form.\n",
        "6. Separate distinct articles clearly with \"###ARTICLE_SEPARATOR###\".\n",
        "7. If no articles containing the keyword are found, state this explicitly.\n",
        "\n",
        "Your output should consist solely of the extracted articles or the statement that no relevant articles were found. Do not include explanations, summaries, or additional commentary unless specifically requested.\n",
        "\n",
        "Maintain a neutral, objective stance throughout the analysis. Focus on accuracy and completeness in your extractions.\n",
        "\n",
        "# Task Instructions\n",
        "Bitte führe die folgenden Schritte aus\n",
        "1. Lese den gesamten Text sorgfältig durch.\n",
        "2. Identifiziere alle Artikel zum Thema Erdbeben, Erdbebenkatastrophe in Italien/Messina/Sizilien/katastrophe/Trümmer und Flüchtlinge und suche nach Keywords 'Erdbeben', 'Erdstoß', Erdbebenkatastrophe', 'Messina', 'Trümmer', 'Hilfsaktion', 'Katastrophe'.\n",
        "3. Für jedes Vorkommen des themas:\n",
        "   a. Bestimme den Anfang des Artikels, in dem Keywords vorkommen.\n",
        "   b. Kontrolliere Satz für Satz, ob diese zusammengehören, Ende den Artikel, wenn die Sätze nicht mehr zusammengehören.\n",
        "   c. Markiere den vollständigen Artikel von Anfang bis Ende.\n",
        "   d. Wenn der Artikel zu lang für eine Antwort ist, schreibe \"[CONTINUED]\" am Ende und setze in der nächsten Antwort fort.\n",
        "4. Überprüfe jeden markierten Artikel:\n",
        "   a. Stelle sicher, dass er eine zusammenhängende Einheit bildet.\n",
        "   b. Vergewissere dich, dass er zum Thema passt.\n",
        "   c. Prüfe, ob er vollständig ist und keine wichtigen Informationen fehlen.\n",
        "5. Extrahiere jeden überprüften Artikel als Originaltext ohne Korrekturen.\n",
        "6. Gib die extrahierten Artikel exakt und unverändert wieder, ohne Zusammenfassungen oder zusätzliche Kommentare.\n",
        "7. Wenn keine Artikel gefunden wurden, gib \"Keine Artikel mit dem angegebenen Keyword gefunden.\" aus.\n",
        "\n",
        "Führe nun diese Schritte für den folgenden Text aus:\n",
        "{text}\n",
        "\"\"\"\n",
        "\n",
        "        articles = []\n",
        "        current_article = \"\"\n",
        "        continuation = False\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                response = client.generate(\n",
        "                    model='nemotron:latest',\n",
        "                    prompt=combined_prompt if not continuation else current_article,\n",
        "                    options={\n",
        "                        'num_ctx': 20000,\n",
        "                        'temperature': 0.1,\n",
        "                        'top_p': 0.5,\n",
        "                        'num_predict': 20000,\n",
        "                        'repeat_penalty': 1,\n",
        "                        'top_k': 20\n",
        "                    }\n",
        "                )\n",
        "\n",
        "                content = response['response']\n",
        "\n",
        "                if continuation:\n",
        "                    current_article += content\n",
        "                else:\n",
        "                    parts = content.split(\"###ARTICLE_SEPARATOR###\")\n",
        "                    for part in parts[:-1]:\n",
        "                        if current_article:\n",
        "                            articles.append({\"article\": current_article.strip()})\n",
        "                            current_article = \"\"\n",
        "                        articles.append({\"article\": part.strip()})\n",
        "                    current_article = parts[-1]\n",
        "\n",
        "                if content.endswith(\"[CONTINUED]\"):\n",
        "                    continuation = True\n",
        "                    current_article = current_article[:-11]  # Remove \"[CONTINUED]\"\n",
        "                else:\n",
        "                    continuation = False\n",
        "                    if current_article:\n",
        "                        articles.append({\"article\": current_article.strip()})\n",
        "                        current_article = \"\"\n",
        "                    break\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in AI processing: {str(e)}\")\n",
        "                break\n",
        "\n",
        "        return articles\n",
        "\n",
        "    # Apply the analysis to each row in the DataFrame\n",
        "    all_articles = []\n",
        "    for index, row in df.iterrows():\n",
        "        articles = analyze_text(row[text_column])\n",
        "        for i, article in enumerate(articles, 1):\n",
        "            new_row = row.to_dict()\n",
        "            new_row['extracted_article'] = article['article']\n",
        "            new_row['article_part'] = i\n",
        "            new_row['total_parts'] = len(articles)\n",
        "            all_articles.append(new_row)\n",
        "\n",
        "    # Create a new DataFrame with individual rows for each article\n",
        "    result_df = pd.DataFrame(all_articles)\n",
        "\n",
        "    return result_df\n",
        "\n",
        "# Usage example (run this in your notebook)\n",
        "text_column = 'plainpagefulltext'\n",
        "result_df = analyze_dataframe(df, text_column)\n",
        "\n",
        "# Optionally, save the results to an Excel file\n",
        "result_df.to_excel('analysis_results.xlsx', index=False)\n",
        "\n",
        "# Display the first few rows of the result\n",
        "print(result_df.head())"
      ],
      "metadata": {
        "id": "WEkXVF7GhCSq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}