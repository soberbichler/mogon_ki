{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNE199nWcbw7pymc+n5gHKf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soberbichler/mogon_ki/blob/main/article_separation_nemotron_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BfyThlPcS7x"
      },
      "outputs": [],
      "source": [
        "import ollama"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['no_proxy'] = 'localhost'\n"
      ],
      "metadata": {
        "id": "xjFPgObKcjty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  client = ollama.Client()\n",
        "  client.list()"
      ],
      "metadata": {
        "id": "q5C6sP_WcmY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Replace 'your_excel_file.xlsx' with the actual path to your Excel file\n",
        "df = pd.read_excel('your_excel_file.xlsx')\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "5qVqySaiizWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from typing import List, Dict\n",
        "\n",
        "with open('examples.txt', 'r') as file:\n",
        "    examples = file.read()\n",
        "\n",
        "def analyze_dataframe(df: pd.DataFrame, text_column: str) -> pd.DataFrame:\n",
        "    def analyze_text(text: str) -> List[Dict[str, str]]:\n",
        "        combined_prompt = f\"\"\"\n",
        "# System Instructions\n",
        "You are an expert text analyst and information retrieval specialist. Use {examples} for structuring your answer.\n",
        "You hate creating summaries and haluzinations,\n",
        "Your task is to carefully analyze given texts and extract complete articles that contain specific keywords. You never create your own content or change original texts.\n",
        "\n",
        "Your output should consist solely of the extracted articles or the statement that no relevant articles were found. Do not include explanations, summaries, or additional commentary unless specifically requested.\n",
        "\n",
        "Maintain a neutral, objective stance throughout the analysis. Focus on accuracy and completeness in your extractions.\n",
        "\n",
        "# Task Instructions\n",
        "Bitte führe die folgenden Schritte aus\n",
        "1. Lese jeden Text aufmerksam durch. Behandle jeden Text als eigene Einheit, ohne auf andere TExte zu refierieren\n",
        "2. Identifiziere alle Artikel mit dem Keyword 'Erdbeben'\n",
        "3. Für jedes Vorkommen eines der Keywords:\n",
        "   a. Bestimme den Anfang des Artikels, in dem Keywords vorkommen.\n",
        "   b. Kontrolliere Satz für Satz, ob diese zusammengehören, Ende den Artikel, wenn die Sätze nicht mehr zusammengehören.\n",
        "   c. Markiere den vollständigen Artikel von Anfang bis Ende.\n",
        "   d. Wenn der Artikel zu lang für eine Antwort ist, schreibe \"[CONTINUED]\" am Ende und setze in der nächsten Antwort fort.\n",
        "    e. Berücksichtige auch sehr kurze und sehr lange Artikel\n",
        "4. Überprüfe jeden markierten Artikel:\n",
        "   a. Stelle sicher, dass er eine Einheit bildet, auch wenn es nicht mehr um Erdbeben geht.\n",
        "   b. Vergewissere dich, dass er eines der gennanten Keywords enthält.\n",
        "   c. Prüfe, ob der extrahierte Text tatsächlich im Dokument ist\n",
        "5. Extrahiere jeden überprüften Artikel als Originaltext, der nichts als den originalen Text enthält, auch keine OCR Korrekturen\n",
        "7. Wenn keine Artikel gefunden wurden, gib \"Keine Artikel mit dem angegebenen Keyword gefunden.\" aus.\n",
        "\n",
        "Führe nun diese Schritte für den folgenden Text aus:\n",
        "{text}\n",
        "\"\"\"\n",
        "\n",
        "        articles = []\n",
        "        current_article = \"\"\n",
        "        continuation = False\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                response = client.generate(\n",
        "                    model='nemotron:latest',\n",
        "                    prompt=combined_prompt if not continuation else current_article,\n",
        "                    options={\n",
        "                         'num_ctx': 30000,\n",
        "                        'temperature': 0.0,\n",
        "                        'num_predict': 20000,\n",
        "                        'repeat_penalty': 1,\n",
        "                        'top_k': 20\n",
        "                    }\n",
        "                )\n",
        "\n",
        "                content = response['response']\n",
        "\n",
        "                if continuation:\n",
        "                    current_article += content\n",
        "                else:\n",
        "                    parts = content.split(\"**END OF ARTICLE**\")\n",
        "                    for part in parts[:-1]:\n",
        "                        if current_article:\n",
        "                            articles.append({\"article\": current_article.strip()})\n",
        "                            current_article = \"\"\n",
        "                        articles.append({\"article\": part.strip()})\n",
        "                    current_article = parts[-1]\n",
        "\n",
        "                if content.endswith(\"[CONTINUED]\"):\n",
        "                    continuation = True\n",
        "                    current_article = current_article[:-11]  # Remove \"[CONTINUED]\"\n",
        "                else:\n",
        "                    continuation = False\n",
        "                    if current_article:\n",
        "                        articles.append({\"article\": current_article.strip()})\n",
        "                        current_article = \"\"\n",
        "                    break\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in AI processing: {str(e)}\")\n",
        "                break\n",
        "\n",
        "        return articles\n",
        "\n",
        "    # Apply the analysis to each row in the DataFrame\n",
        "    all_articles = []\n",
        "    for index, row in df.iterrows():\n",
        "        articles = analyze_text(row[text_column])\n",
        "        for i, article in enumerate(articles, 1):\n",
        "            new_row = row.to_dict()\n",
        "            new_row['extracted_article'] = article['article']\n",
        "            new_row['article_part'] = i\n",
        "            new_row['total_parts'] = len(articles)\n",
        "            all_articles.append(new_row)\n",
        "\n",
        "    # Create a new DataFrame with individual rows for each article\n",
        "    result_df = pd.DataFrame(all_articles)\n",
        "\n",
        "    return result_df\n",
        "\n",
        "# Usage example (run this in your notebook)\n",
        "text_column = 'plainpagefulltext'\n",
        "result_df = analyze_dataframe(df, text_column)\n",
        "\n",
        "# Optionally, save the results to an Excel file\n",
        "result_df.to_excel('analysis_results.xlsx', index=False)\n",
        "\n",
        "# Display the first few rows of the result\n",
        "print(result_df.head())"
      ],
      "metadata": {
        "id": "WEkXVF7GhCSq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}